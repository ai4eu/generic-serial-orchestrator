#!/usr/bin/env python3
# ===================================================================================
# Copyright (C) 2021 Fraunhofer Gesellschaft. All rights reserved.
# ===================================================================================
# This Acumos software file is distributed by Fraunhofer Gesellschaft
# under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# This file is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===============LICENSE_END========================================================

"""The merged protobuf for the pipeline needs to be named pipeline.proto"""
import json
from extractjson import json_extract
from queue import Queue
import grpc
import importlib
from google.protobuf import empty_pb2
from typing import ItemsView, List, Any


class Operation(object):
    """A Class for storing details about Operations in pipeline"""
    node_name = ""
    protobuf_loc = ""
    service_name = ""
    input_msg = ""
    output_msg = ""
    connected_to = []
    port = ""
    stub = ""

    def __init__(self, node_name, protobuf_loc, service_name, input_msg, output_msg, connected_to):
        self.node_name = node_name
        self.protobuf_loc = protobuf_loc
        self.service_name = service_name
        self.input_msg = input_msg
        self.output_msg = output_msg
        self.connected_to = connected_to

    def identifier(self) -> str:
        return '{}/{}'.format(self.node_name, self.service_name)


class InputNode(object):
    """A Simple Class for Input Node of the Pipeline"""
    node_name = ""


class PipelineReader(object):
    """This class is responsible for reading the dockerinfo and blueprint files generated by the AcuCompose Validator and
    getting all the useful details about ech Node in a pipeline"""
    data = {}  # blueprint json
    port_info = {}  # dockerinfo json
    operations = []  # flattened list of operations
    total_operations = 0

    def __init__(self, path, dockerinfo_path):
        print('PipelineReader reading from', path, 'and', dockerinfo_path)
        with open(path) as f:
            self.data = json.load(f)

        # create a flattened representation of operations
        self.operations = [
            Operation(
                node['container_name'],
                node['proto_uri'],
                operation_signature['operation_signature']['operation_name'],
                operation_signature['operation_signature']['input_message_name'],
                operation_signature['operation_signature']['output_message_name'],
                operation_signature['connected_to'])
            for node in self.data['nodes']
            for operation_signature in node['operation_signature_list']]
        self.total_operations = len(self.operations)

        with open(dockerinfo_path) as f:
            self.port_info = json.load(f)
        # transform dockerinfo json into dict container_name -> original dict parts
        self.port_info = {
            item['container_name']: item
            for item in self.port_info['docker_info_list']}

    def get_input_operation(self):
        EMPTIES = frozenset(['Empty', 'google.protobuf.Empty'])
        operations_with_empty_input = [
            operation
            for operation in self.operations
            if operation.input_msg in EMPTIES]

        if len(operations_with_empty_input) != 1:
            raise Exception("this orchestrator can only process pipelines with one data provider = one operation with Empty input, found %d" % len(operations_with_empty_input))

        return operations_with_empty_input[0]

    def get_operation(self, i) -> Operation:
        return self.operations[i]

    def get_node_port(self, i):
        """extract node port and stub information from dockerinfo.json"""
        value6 = self.port_info[self.operations[i].node_name]['port']
        value7 = self.port_info[self.operations[i].node_name]["ip_address"]
        port = str(value7) + ":" + str(value6)
        return port

    def get_adjacent_operation_identifiers(self, i):
        values = self.operations[i].connected_to
        identifiers = [
            '{}/{}'.format(c['container_name'], c['operation_signature']['operation_name'])
            for c in values]
        print("get_adjacent_operation_identifiers(%d) returns %s" % (i, identifiers))
        return identifiers


class Graph:
    """Class for Graph Data Structure"""

    def __init__(self, operations: List[Operation]):
        self.operations = operations
        self.adj_list = {}

        for o in self.operations:
            self.adj_list[o.identifier()] = []

    def print_adj_list(self):
        for operation in self.operations:
            print(operation.identifier(), "->", self.adj_list[operation.identifier()])

    def add_edge(self, u, v):
        self.adj_list[u].append(v)

    def degree(self, node):
        deg = len(self.adj_list[node])
        return deg


class GenericOrchestrator:
    """Class for executing the pipeline"""

    def bfs_traversal(self, adj_list, input_node):
        """This Function implements the BFS algorithm on a adjacency list of a graph
        This is intended to support branches for parallel paths in a pipeline

        Input: adjacency list, starting Node of the graph
        Output: returns a BFS Traversal path
        """
        visited = {}
        level = {}  # distance dictionary
        parent = {}
        bfs_traversal_output = []
        queue = Queue()

        for node in adj_list.keys():
            visited[node] = False
            parent[node] = None
            level[node] = -1

        print(visited)

        source = input_node
        visited[source] = True
        level[source] = 0
        parent[source] = None

        queue.put(source)

        while not queue.empty():
            u = queue.get()
            bfs_traversal_output.append(u)

            for v in adj_list[u]:
                if not visited[v]:
                    visited[v] = True
                    parent[v] = u
                    level[v] = level[u] + 1
                    queue.put(v)

        return bfs_traversal_output

    def find_node_in_pipeline(self, node, pipeline, p):
        """Function to find the index of the node the pipeline given the node name
        :argument:
            node(str): Nodename
            pipeline(List): pipeline instance which contains list of node properties
        :returns
            index_to_return(int): Index of the node in list pipeline
        """
        for i in range(p.total_Nodes):

            if node == pipeline[i].node_name:
                index_to_return = i
                break

        return index_to_return

    def find_stub_for_node(self, stubs, pipeline, p, rpc_service_map):
        """Function to find the right stubs for a node"""
        for i in range(p.total_operations):
            for j in range(len(stubs)):
                if rpc_service_map[pipeline[i].service_name] == stubs[j]:
                    # TODO handle multiple containers with same service name gracefully (this is currently broken/not foreseen)
                    pipeline[i].stub = stubs[j]
                    # print("find_stub_for_node assigns", stubs[j], 'to', pipeline[i].identifier())

    def get_all_stubs(self, path):
        """Function to get all the stubs from the combined protofile
        :arg
            path(str): path of pipeline_pb2_grpc.py
        :returns
            stubs(List): List containing the stubs
        """
        stubs = []
        for line in open(path, "r"):
            if "Stub" in line:
                split1 = line.split()
                split2 = split1[1].split('(')
                stub_str = split2[0]
                stubs.append(stub_str)
                # stubs[pipeline[i].node_name] = stub_str
        # print('get_all_stubs returns', stubs)
        return stubs

    def start_node(self, i, pipe_line, p):
        """A generic Function to get port,stub,request and response for each node
        Input: Index of the node
        Output: Port address, stub method, request method, response method"""

        pb2 = importlib.import_module("work_dir.pipeline_pb2")
        pb2_grpc = importlib.import_module("work_dir.pipeline_pb2_grpc")

        port = pipe_line[i].port
        channel = grpc.insecure_channel(port)
        # create a stub (client)
        stub_method = getattr(pb2_grpc, pipe_line[i].stub)
        stub = stub_method(channel)

        if pipe_line[i].input_msg == "google.protobuf.Empty" or pipe_line[i].input_msg == "Empty" or \
                pipe_line[i].input_msg == "empty":
            request_method = empty_pb2.Empty
        else:
            request_method = getattr(pb2, pipe_line[i].input_msg)

        response_before_call = getattr(pb2, pipe_line[i].output_msg)

        response_method = getattr(stub, pipe_line[i].service_name)

        return request_method, response_method, response_before_call

    def link_nodes(self, bfs_list, pipe_line, p, num_nodes, current_node=0, previous_response=None):
        """A Recursive function to link nodes in a pipeline
        :arg
            bfs_list(List): List containing BFS traversal for the graph created
            pipe_line(List): List of Node objects
            p(object): Instance of class PipelineReader
            stubs(List): List of stubs returned by get_all_stubs
            current_node(int): Index of current node
            previous_response: response of the previous node
        """

        i = self.find_node_in_pipeline(bfs_list[current_node], pipe_line, p)
        request_method, response_method, response_before_call = self.start_node(i, pipe_line, p)

        if previous_response is None:
            """This is for the first Node i.e Databroker in the pipeline which according to ai4eu container 
            specification has empty input type"""
            request_1 = request_method()
            response_1 = response_method(request_1)
            response_before = response_before_call()
        else:
            response_before = response_before_call()
            response_1 = response_method(previous_response)

        print("*********************************************")
        print("Response of node", current_node)
        print(response_before)
        print(response_1)

        """Our termination case out of recursive function is when response before grpc call is equal to 
         response after grpc call!!!"""
        if response_before != response_1:
            """Increment the count of current node and call the function recursively until all nodes are exhausted"""
            current_node = current_node + 1
            if current_node == num_nodes:
                """continue message dispatching through the pipeline until data broker(Input Node) is exhausted"""
                current_node = 0
            self.link_nodes(bfs_list, pipe_line, p, num_nodes, current_node=current_node,
                            previous_response=response_1)

    # this is the main (only) entry point into this class!
    def execute_pipeline(self, blueprint, dockerinfo, rpc_service_map):
        """Start Pipeline Excecution
        :arg
            blueprint(string): Path of blueprint.json
            dockerinfo(string): Path of dockerinfo.json
        """

        # print('rpc_service_map=', rpc_service_map)
        # stubs = self.get_all_stubs("work_dir/pipeline_pb2_grpc.py")

        """create pipeline reader instance"""
        p = PipelineReader(blueprint, dockerinfo)

        """create a list of node instances"""
        pipe_line = p.operations

        print("Total number of nodes in pipeline", p.total_operations)

        """create a instance of class Graph"""
        graph = Graph(pipe_line)

        print("adjacency list before adding edges")
        graph.print_adj_list()

        """Add edges to the graph"""
        for i in range(p.total_operations):
            for adj in p.get_adjacent_operation_identifiers(i):
                graph.add_edge(p.get_operation(i).identifier(), adj)

        print("\n")
        print("adjacency list after adding edges")
        graph.print_adj_list()

        """get all node properties"""
        for i in range(p.total_operations):
            pipe_line[i].port = p.get_node_port(i)

        print("\n")

        input_operation = p.get_input_operation()

        stubs = self.get_all_stubs("work_dir/pipeline_pb2_grpc.py")

        self.find_stub_for_node(stubs, pipe_line, p, rpc_service_map)

        """Graph Traversal using BFS Algorithm"""
        bfs_list = self.bfs_traversal(graph.adj_list, input_node.node_name)

        print("bfs output:", bfs_list)
        num_nodes = p.total_Nodes

        """Using Recursion to link nodes in a pipeline"""
        self.link_nodes(bfs_list, pipe_line, p, num_nodes, current_node=0, previous_response=None)
